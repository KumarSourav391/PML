{"cells":[{"cell_type":"markdown","metadata":{"id":"MyOkxWhumI7S"},"source":["# Credit Card Fraud Detection\n","It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase."]},{"cell_type":"markdown","metadata":{"id":"hI5Df6k_mI7V"},"source":["## Description of data\n","\n","- The datasets contains transactions made by credit cards in September 2013 by european cardholders.\n","- This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions.\n","- The dataset is highly <b>unbalanced</b>, the positive class (frauds) account for 0.172% of all transactions.\n","- It contains only numerical input variables which are the result of a PCA transformation. \n","- Due to confidentiality issues, the original features and more background information about the data is not provided. \n","- Features V1, V2, â€¦ V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. \n","- Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset.\n","- The feature 'Amount' is the transaction Amount.\n","- Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n","\n","Source: [Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud)"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"otf71mLTmI7X"},"source":["## Import required Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"29cODvd-mI7Y"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","# For scaling the features and train-test split\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n","\n","# For model buidling\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from xgboost import XGBClassifier\n","from sklearn.svm import SVC\n","\n","# For hyper-paramter tuning\n","# from sklearn.model_selection import GridSearchCV\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from utils import predict_and_evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"Q9xs_aEwmI7Z","outputId":"43031a13-cdaa-4187-bc2f-6e41a5d368fa"},"outputs":[{"ename":"OSError","evalue":"Invalid data stream","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[1;32m<ipython-input-3-695b2aab4c5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# read data file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# this file is compressed in bzip2 format and index column is included in it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CC.csv.bz2\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bz2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1055\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1057\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1058\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2059\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2060\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2061\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2062\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2063\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n","\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n","\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n","\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n","\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\bz2.py\u001b[0m in \u001b[0;36mread1\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"B\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\_compression.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                     \u001b[0mrawblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrawblock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mOSError\u001b[0m: Invalid data stream"]}],"source":["# read data file\n","# this file is compressed in bzip2 format and index column is included in it\n","df = pd.read_csv(\"CC.csv.bz2\",compression='bz2', index_col=0)"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"TIT9x9sVmI7b"},"source":["##  Undersand the data"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"tags":[],"id":"kKXMNOCwmI7b"},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"BdZnFLvLmI7c"},"outputs":[],"source":["df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"tags":[],"id":"jOTAxHvpmI7d"},"outputs":[],"source":["df.isnull().sum() # Check Null Values!"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"tags":[],"id":"i2wKAHF3mI7e"},"outputs":[],"source":["df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"YYYRH7eJmI7e"},"outputs":[],"source":["# Check Distribution Of Label\n","print('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\n","print('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"tags":[],"id":"qGZT9C8-mI7f"},"outputs":[],"source":["# The classes are heavily skewed. This is problem that needs to be solved. How?\n","print('No Frauds', round(df['Class'].value_counts()[0],2), 'are normal transactions')\n","print('Frauds', round(df['Class'].value_counts()[1],2), 'are fraud')"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"SanWbjrKmI7f"},"outputs":[],"source":["colors = [\"#0101DF\", \"#DF0101\"]\n","\n","sns.countplot('Class', data=df, palette=colors)\n","plt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"ngzMWiS2mI7g"},"source":["- Notice how imbalanced is our original dataset! \n","- Most of the transactions are non-fraud. \n","- If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud. \n","- But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"2jladhmYmI7g"},"source":["## Preprocessing - Scaling and Distribution\n","- We will first scale the columns comprise of <b>Time</b> and <b>Amount </b>. \n","- Time and amount should be scaled as the other columns. \n","- On the other hand, we need to also create a sub sample of the dataframe in order to have an equal amount of Fraud and Non-Fraud cases, helping our algorithms better understand patterns that determines whether a transaction is a fraud or not.\n","\n","### What is a sub-Sample?\n","In this scenario, our subsample will be a dataframe with a 50/50 ratio of fraud and non-fraud transactions. Meaning our sub-sample will have the same amount of fraud and non fraud transactions.\n","\n","### Why do we create a sub-Sample?\n","We saw that the original dataframe is heavily imbalanced! Using the original dataframe  will cause the following issues:\n","<ul>\n","<li><b>Overfitting: </b>Our classification models will assume that in most cases there are no frauds! What we want for our model is to be certain when a fraud occurs. </li>\n","<li><b>Wrong Correlations:</b> Although we don't know what the \"V\" features stand for, it will be useful to understand how each of this features influence the result (Fraud or No Fraud) by having an imbalance dataframe we are not able to see the true correlations between the class and features. </li>\n","</ul>"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"dn9uzElsmI7h"},"source":["### Scaling\n","\n","The **StandardScaler** assumes your data is normally distributed within each feature and will scale them such that the distribution is now centred around 0, with a standard deviation of 1. \n","\n","$$\\frac{\\text{x}-\\text{mean}}{\\text{standard deviation}}$$\n","\n","The **MinMaxScaler** is the probably the most famous scaling algorithm, and follows the following formula for each feature. \n","\n","$$\\frac{\\text{x}-\\text{min}}{\\text{max}-\\text{min}}$$\n","\n","It essentially shrinks the range such that the range is now between 0 and 1 (or -1 to 1 if there are negative values). If the distribution is not Gaussian or the standard deviation is very small, the min-max scaler works better. However, it is sensitive to outliers, so if there are outliers in the data, you might want to consider the Robust Scaler below.\n","\n","**Robust Scaler** scale features using statistics that are robust to outliers. The RobustScaler uses a similar method to the Min-Max scaler but it instead uses the interquartile range, rathar than the min-max, so that it is robust to outliers. \n","\n","$$\\frac{\\text{x}-\\text{Q1(x)}}{\\text{Q3(x)}-\\text{Q1(x)}}$$"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"cHrJdxM-mI7h"},"outputs":[],"source":["# Since most of our data has already been scaled, we will scale the columns that are not scaled (Amount and Time)\n","# RobustScaler is less prone to outliers.\n","rob_scaler = RobustScaler()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I7d5qZNLmI7i","outputId":"8a0fdc99-7f4e-43e5-b8c4-42df5fe321a2"},"outputs":[{"ename":"NameError","evalue":"name 'df' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-1-c42a15b2c7cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"]}],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"A6ggJPJvmI7i"},"outputs":[],"source":["df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"FMVlNEJImI7j"},"outputs":[],"source":["df.drop(['Amount'], axis=1, inplace=True) # remove original time and Amount Columns from df"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"C2NQNENGmI7j"},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"LARW0WMWmI7j"},"outputs":[],"source":["# Rearranging the columns\n","scaled_amount = df['scaled_amount']\n","\n","df.drop(['scaled_amount'], axis=1, inplace=True)\n","df.insert(0, 'scaled_amount', scaled_amount)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"u_qDctUWmI7j"},"outputs":[],"source":["# Amount is Scaled!\n","df.head()"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"Hxdrs5_3mI7k"},"source":["**EXERCISE:** Scale the Time Column"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"hC6c8uD_mI7k"},"source":["### Splitting the DataFrame\n","\n","Before proceeding with any <b> Sampling technique</b> we have to separate the orginal dataframe.<br> \n","<b> Why? for testing purposes, we want to test our models on the original testing set not on the testing set created by either of these techniques.</b><br> The main goal is to fit the model either with the dataframes that were undersample and oversample (in order for our models to detect the patterns), and test it on the original testing set. "]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"6sYyG4ZXmI7k"},"outputs":[],"source":["ss = StratifiedShuffleSplit(n_splits=1,\n","                            test_size=0.2,\n","                            train_size=0.8,\n","                            random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"pxcM0vFFmI7k"},"outputs":[],"source":["X = df.drop('Class', axis=1)\n","y = df['Class']"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"cvuHlluBmI7l"},"outputs":[],"source":["for train_index, test_index in ss.split(X, y):\n","    train_df = df.iloc[train_index]\n","    test_df = df.iloc[test_index]"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"j-o3kqaNmI7l"},"outputs":[],"source":["print('Distributions: \\n')\n","print(\"Train Set\")\n","print(train_df.Class.value_counts())\n","print(\"\\nTest Set\")\n","print(test_df.Class.value_counts())\n","print(\"\\nPercentage:\")\n","print(\"\\nTrain Set\")\n","print((train_df.Class.value_counts()/ len(train_df))*100)\n","print(\"\\nTest Set\")\n","print((test_df.Class.value_counts()/ len(test_df))*100)"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"2BC8Efb4mI7l"},"source":["### Random Under-Sampling:\n","\n","Implement *\"Random Under Sampling\"* which basically consists of removing data in order to have a more <b> balanced dataset </b> and thus avoiding our models to overfitting.\n","\n","**Steps:**\n","<ul>\n","<li>The first thing we have to do is determine how <b>imbalanced</b> is our class (use \"value_counts()\" on the class column to determine the amount for each label)  </li>\n","<li>Once we determine how many instances are considered <b>fraud transactions </b> (Fraud = \"1\") , we should bring the <b>non-fraud transactions</b> to the same amount as fraud transactions (assuming we want a 50/50 ratio), this will be equivalent to 492 cases of fraud and 492 cases of non-fraud transactions.  </li>\n","<li> After implementing this technique, we have a sub-sample of our dataframe with a 50/50 ratio with regards to our classes. Then the next step we will implement is to <b>shuffle the data</b> to see if our models can maintain a certain accuracy everytime we run this script.</li>\n","</ul>\n","\n","**Note:** The main issue with \"Random Under-Sampling\" is that we run the risk that our classification models will not perform as accurate as we would like to since there is a great deal of <b>information loss</b> (randomly picking 394 non-fraud transaction  from 2,27,451 non-fraud transactions)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"T8OU3UKbmI7l"},"outputs":[],"source":["# Lets shuffle the data before creating the subsamples\n","train_df = train_df.sample(frac=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"UgQCR9FdmI7l"},"outputs":[],"source":["# amount of fraud classes 394 rows\n","fraud_df = train_df.loc[train_df['Class'] == 1]\n","non_fraud_df = train_df.loc[train_df['Class'] == 0][:394]"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"oDn2QrM1mI7m"},"outputs":[],"source":["normal_distributed_df = pd.concat([fraud_df, non_fraud_df])"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"lbnIE9nmmI7m"},"outputs":[],"source":["# As fraud_df and non_fraud_df are concatenated, Shuffle dataframe rows to mix the rows\n","df2 = normal_distributed_df.sample(frac=1, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"ZGarXpFbmI7m"},"outputs":[],"source":["df2.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"Jcg5g2wbmI7m"},"outputs":[],"source":["df2.head()"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"c9HMWgcImI7m"},"source":["###  Equally Distributing \n","<a id=\"correlating\"></a>\n","Now that we have our dataframe correctly balanced, we can go further with our <b>analysis</b> and <b>data preprocessing</b>."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"J4Dawo02mI7n"},"outputs":[],"source":["print('Distribution of the Classes in the subsample dataset')\n","print(df2['Class'].value_counts()/len(df2))"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"_m25L--FmI7n"},"outputs":[],"source":["colors = [\"#0101DF\", \"#DF0101\"]\n","sns.countplot('Class', data=df2, palette=colors)\n","plt.title('Equally Distributed Classes', fontsize=14)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"7DDrjJvEmI7n"},"source":["## Training the ML Model for Fraud Detection(Classification) "]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"-95Ai_h1mI7n"},"outputs":[],"source":["# Create X_train, X_test, y_train, y_test for ease of use\n","X_train = df2.drop('Class', axis=1)\n","y_train = df2['Class']\n","\n","X_test = test_df.drop('Class', axis=1)\n","y_test = test_df['Class']"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"AzwNXe63mI7n"},"source":["### Random Forest"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"w2-sblhVmI7n"},"outputs":[],"source":["rf_clf = RandomForestClassifier(n_estimators=100, criterion=\"entropy\")"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"cAdAJQitmI7o"},"outputs":[],"source":["rf_clf.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"Ll_UGAc6mI7o"},"outputs":[],"source":["# Decision Tree in the Forest\n","rf_clf.estimators_[0]"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"X6mtXoBBmI7o"},"source":["#### Evaluation Metrics\n","The Given the class imbalance ratio, Confusion matrix and accuracy is not meaningful\n","for unbalanced classification. A robust evaluation is required to measure the\n","performance of a fraud detection model.\n","\n","**1. False Positives:**\n","A false positive is an outcome where the model incorrectly predicts the positive class.\n","<br>**2. False Negatives:**\n","A false negative is an outcome where the model incorrectly predicts the negative class.\n","<br>**3. Precision:**\n","Precision talks about how precise/accurate the model is i.e. out of those predicted positives, how many of them are actual positive. Precision is a good measure to determine, when the costs of False Positives is high. For instance, here, a false positive means that a transaction is that is non- fraudulent has been identified as fraudulent. This can happen if the precision is not high for the fraud detection model.\n","<br>**4. Recall:**\n","Recall calculates how many of the Actual Positives our model captures through labeling it as Positive (True Positive). If a fraudulent transaction is predicted as non-fraudulent (Predicted Negative), the consequence can be very bad for the bank.\n","<br>**5. F1 Score:**\n","F1 Score is used to seek a balance between Precision and Recall.\n","<br>**6. Mathews Correlation Coefficient:**\n","The coefficient takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between âˆ’1 and +1.<br> \n","A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and âˆ’1 indicates total disagreement between prediction and observation.<br>\n","The Matthews correlation coefficient is more informative than F1 score and\n","accuracy in evaluating binary classification problems, because it takes into\n","account the balance ratios of the four confusion matrix categories (true\n","positives, true negatives, false positives, and false negative)."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"vdzXtdqYmI7o"},"outputs":[],"source":["rf_res = predict_and_evaluate(rf_clf, X_test, y_test)"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"ITUVIHGkmI7o"},"source":["#### Feature Importances\n","\n","In order to quantify the usefulness of all the variables in the entire random forest, we can look at the relative importances of the variables."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"0Uat8w8WmI7p"},"outputs":[],"source":["rf_clf.feature_importances_"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"FLnnvyJZmI7p"},"outputs":[],"source":["feature_importances = pd.Series(rf_clf.feature_importances_, index=X_train.columns)\n","feature_importances.sort_values(ascending=False, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"7hfgs-T3mI7p"},"outputs":[],"source":["feature_importances"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"SE91kfcimI7p"},"outputs":[],"source":["fig = plt.figure(figsize=(8,4), dpi=100)\n","feature_importances.plot.bar()\n","plt.title(\"Feature importances using MDI\")\n","plt.xlabel(\"Features\")\n","plt.ylabel(\"Mean Decrease in Impurity\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"FnwepmiBmI7p"},"source":["**EXERCISE:** Train RF model again by considering only important features (e.g. top 10) and evaluate the model and observe the difference in the metrics."]},{"cell_type":"markdown","metadata":{"tags":[],"id":"2yTWOmNJmI7p"},"source":["### Gradient Boosting"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"9iscWIAimI7p"},"outputs":[],"source":["gbm_clf = GradientBoostingClassifier()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"S2-0HZPAmI7q"},"outputs":[],"source":["gbm_clf.fit(X_train,y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"XhLmU3vgmI7q"},"outputs":[],"source":["gbm_res = predict_and_evaluate(gbm_clf, X_test, y_test)"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"qa65kl8RmI7q"},"source":["### XGBoost"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"68gGJAcumI7q"},"outputs":[],"source":["xgb_clf = XGBClassifier()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"lC_Fl4UWmI7q"},"outputs":[],"source":["xgb_clf.fit(X_train,y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"iIhLtUpHmI7q"},"outputs":[],"source":["xgb_res = predict_and_evaluate(xgb_clf, X_test, y_test)"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"W5TVu5JwmI7q"},"source":["### SVM"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"SBfsYR7TmI7r"},"outputs":[],"source":["svm_clf = SVC()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"xBgdgw4EmI7r"},"outputs":[],"source":["svm_clf.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"4GnX35KhmI7r"},"outputs":[],"source":["svm_res = predict_and_evaluate(svm_clf, X_test, y_test)"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"1rHAjhKvmI7r"},"source":["## Comparing the metrics for all the algorithms"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"AbcAw-XWmI7r"},"outputs":[],"source":["results = pd.DataFrame(data=[rf_res, gbm_res, xgb_res, svm_res], \n","             columns=('Algorithm','False Positives', \n","                      'False Negatives', 'Precision', \n","                      'Recall', 'F1 Score', 'MCC'))"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"OyqqvJs0mI7r"},"outputs":[],"source":["results"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"toc-autonumbering":true,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}